\chapter{Code Generation}\label{chapter:generating}
% \begin{itemize}
% 	\item Compilation scheme?
% 	\item How is data represented? Lists tuples
% 	\item Semantics style, call-by-reference, call-by-value?
% 	\item How did you solve overloaded functions?
% 	\item Polymorphism?
% 	\item Printing?
% 	\item Problems?
% 	\item\ldots
% \end{itemize}

\section{Representation}

\subsection{Basic Values}
For our compiler, we chose to store values of type @Int@, @Bool@, or @Char@ as one word on the stack. When needed elsewhere, their value is simply copied. This means that values of these types are passed by value to other functions.
% Tuples and lists, on the other hand, are stored as the heap address of the first item within them. When these are needed elsewhere, only the address is copied. This means that basic values are passed by value, while compound values are passed by reference.

\subsection{Lists}
Contrary to basic values, lists are stored on the heap, and the address of the first item is stored on the stack. This implies that lists are passed by reference to other functions.

The empty list @[]@ is represented as address 0, and given any list @list@, prepending a value to the list is done by storing its address, followed by the value, on the heap, and then changing the address of the list to the address of the latter heap location.

\subsection{Tuples}
Despite the fact that the size of tuples can be known at compile time, as long as they do not directly or indirectly contain lists, tuples are also stored on the heap. This makes handling their locations in memory easier, and allows us to simplify the code generation because all values are exactly one word in size, which implies non-overloaded polymorphic functions do not require specific variants for every type. This also means that tuples are passed by reference, just like lists, and so calling functions with tuples as arguments might change their contents.

Storing tuples is done by first storing the right and then the left inner value on the heap, and pushing the latter heap address to the stack.

\section{Global Approach}
The code generation of the compiler works as follows:
\begin{enumerate}
    \item The very first code we generate and execute is to reserve space for global variables. This is done by counting the number of top-level variable declarations in the decorated abstract syntax tree, and linking that number of words, as every value takes up one word of space. When we pushed enough empty values on the stack, we copy the stack pointer to scratch register R7, which will be our dedicated Globals Pointer (GP) from now on. Much like the Mark Pointer can be used to easily find local variables and arguments in the local scope using a constant offset, the GP allows us to find the memory location of global variables using their index as offset.
    \item Next, we generate code for initializing these variables. We loop through all top-level variable declarations in the tree, and generate the code for their expressions, which leaves their value on the stack. We then move this value to the spot reserved for this declaration in the previous step. Since the variable declarations are topologically sorted and cannot contain cycles, this step cannot fail, provided that the program passed the type checker.
    \item The next step is to generate code for core functions. These are functions that cannot be expressed in SPL with basic types alone, such as all operators, fields, and @print@ variants for basic types. These implementations are quite straightforward, as most operators are single instructions for the SSM.
    \item Now we branch to the @main :: -> Void@ function. We generate this function as follows:
    \begin{enumerate}
        \item We start by reserving space for local variables. Since SPL functions declare all their local variables at the beginning of a function, this is as simple as linking the number of variables.
        \item Then we generate code for initializing these variables, and move the result to the reserved spot, similar to global variables.
        \item Next, we generate code for each of the statements in the function. This is straightforward.
        \item Every time we encounter a function call that is not yet generated, we remember the function call for the next step.
        \item Finally, we unlink the local variables and return to the caller. The arguments of the function are pushed by the caller before the function is called, and it is the callers job to remove them afterwards as well.
    \end{enumerate}
    When the @main@ function returns, we halt the program. If no @main@ function is found, the generator throws an error.
    \item As described in the previous step, we keep track of function calls that exist but have not been generated yet. The next step is to generate each of these, and recursively their function calls as well, until no more are left. Generating the appropriate function for a function call is done by looking at all function declarations until the one with the same name as the call is found, and generating a variant of that function using the concrete types of the type arguments.
    
    To illustrate, take the following function as an example:
\begin{lstlisting}
pront(x) :: Show a => a -> Void {
    print(x);
}
\end{lstlisting}
    In this context, a function call @pront(1)@ would be annotated with @:: Int@, as explained in chapter \ref{chapter:typing}. Such a function call will trigger the generator to call and generate the \textit{monomorphized} function variant @pront-tInt(x) :: Int -> Void@, which in turn calls @print-tInt(x)@, which is a core function that instructs the machine to print @x@ as an integer. Likewise, there are core functions @print-tBool(x)@ and @print-tChar(x)@ to print types as booleans (``True'' or ``False'') or characters respectively.
%     To illustrate, recall the function from chapter \ref{chapter:typing}:
% \begin{lstlisting}
% f(a, b) Ord a => a a -> Bool {
%     return a < b;
% }
% \end{lstlisting}
    % the call @f(5, 7)@ is annotated with @:: Int@. When this function call is generated, we create the \textit{non-overloaded} variant @f-tInt(x) Int Int -> Bool { return x; }@.
    % This technique of generating a new variant for each combination of type arguments is called \textit{monomorphization}.
    
    Note that, for purely polymorphic functions, such as the identity function:
\begin{lstlisting}
id(x) :: a -> a {
    return x;
}
\end{lstlisting}
   generating monomorphized variants like @id-tInt@ and @id-tChar@ would result in duplicate code, as all polymorphic functions treat basic values and addresses the same, and all values have the same size. As such, it is a waste to monomorphize these, which is why only type variables with type classes imposed on them should have their functions monomorphized, because only those are used in overloaded function calls. In a language that does not box all non-basic types, such as Rust, these functions would be monomorphized as well, because returning a \lstinline[language=rust]|char| results in one byte, while a tuple \lstinline[language=rust]|(i32, u32)| yields eight, both requiring different code.
    % In case of \textit{overloaded} functions such as @print@, however, it is necessary to know the concrete type, so that the proper variant is called in the generated code. For example, the following function would not work properly if it was not monomorphized:

    % This is because @pront(3)@ should print ``3'', while @pront(True)@ should print ``True''. Hence, the @print()@ call is different in each variant. A working generic approach that is not aware of the concrete type of @x@ would instead print something along the lines of ``3'' and ``-1'', respectively.
    
    % It is relatively simple to check whether or not a function is overloaded, and should therefore be monomorphized, by recursively checking if any of the function calls within it are overloaded. This would reduce the amount of duplicate generated code. For now, though, we did not make this distinction.
\end{enumerate}

\section{Polymorphism}
Since the @main@ function itself is not polymorphic, and every function is called with concrete arguments, none of the function calls will have polymorphic type arguments, and so generating variants of polymorphic functions is straightforward. Unfortunately, there is one exception: the empty list @[]@ has type @[a]@, which introduces a new type variable. If no items are added to the list, this type variable will remain abstract and will complicate generating code for overloaded functions. We can simply solve this problem by raising an exception if a overloaded function variant is generated with a polymorphic argument. When the user encounters this error, all they need to do to get rid of it is annotate the empty list in question with a type.

\section{Overloading}
Currently, our compiler only supports overloading for functions on basic types, because monomorphization automatically takes care of this problem. For example, the expression @1 == 1@ is translated to a function call to @eq-tInt()@ during the code generation, whereas the expression @'a' == 'b'@ calls @eq-tChar()@ instead. Since these constant variants are generated for all basic types during the core functionality generation stage, they are already properly overloaded. This is different for lists and tuples, because we cannot pre-generate all variants for any type of list or tuple: there are simply too many variants. We could create core variants for @[Int]@, @[Bool]@, and @[Char]@, but then it still would not work for @[[Int]]@, for instance.

The solution is to have generic implementations that work for any type of list, and generate necessary variants from that implementation when they are needed. To support this, we will attempt to implement \textit{type classes}. How this works exactly is described in detail in the next chapter.
